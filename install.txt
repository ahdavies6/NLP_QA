Required packages: NLTK, numpy
    NLTK: conda install -c anaconda nltk
    numpy: conda install -c anaconda numpy
    gensim: conda install -c conda-forge gensim
    spaCy: conda install -c conda-forge spacy
        (optional)

NLTK parse stuff:
    """
    import nltk

    for x in ["words", "tagsets", "averaged_perceptron_tagger", "wordnet"]:
        nltk.download(x)
    """

Stanford-CoreNLP:
    download: https://stanfordnlp.github.io/CoreNLP/download.html
    extract, cd to folder, then
    run: java -Xmx4096M -cp "*" edu.stanford.nlp.pipeline.StanfordCoreNLPServer \
            -preload tokenize,ssplit,pos,lemma,ner,parse,depparse \
            -status_port 9000 -port 9000 -timeout 15000 &

Stanford-NER:
    https://nlp.stanford.edu/software/CRF-NER.html#Download
    extract
    rename extracted dir to 'stanford-ner'

spaCy (download English models):
    python -m spacy download en

Instructions for text_analyzer:
    all sentence functions currently the full question, untokenized.
    Conversion for use with parse classes is needed.
    use get_prospects_for_who() for who questions
    use get_prospects_for_where() for where questions
    use get_prospects_with_lemmatizer2() for others, for now.
